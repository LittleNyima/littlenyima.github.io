<!DOCTYPE html><html lang="zh-TW" data-theme="light"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1.0,viewport-fit=cover"><title>極東晝寢愛好家 - 晝寢日和(@w@)zzz</title><meta name="author" content="LittleNyima"><meta name="copyright" content="LittleNyima"><meta name="format-detection" content="telephone=no"><meta name="theme-color" content="#ffffff"><meta property="og:type" content="website">
<meta property="og:title" content="極東晝寢愛好家">
<meta property="og:url" content="https://littlenyima.github.io/index.html">
<meta property="og:site_name" content="極東晝寢愛好家">
<meta property="og:locale" content="zh_TW">
<meta property="og:image" content="https://littlenyima.github.io/img/avatar.jpg">
<meta property="article:author" content="LittleNyima">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://littlenyima.github.io/img/avatar.jpg"><link rel="shortcut icon" href="/img/favicon64x64.png"><link rel="canonical" href="https://littlenyima.github.io/index.html"><link rel="preconnect" href="//cdn.jsdelivr.net"/><link rel="preconnect" href="//busuanzi.ibruce.info"/><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free/css/all.min.css" media="print" onload="this.media='all'"><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.min.css" media="print" onload="this.media='all'"><script>const GLOBAL_CONFIG = {
  root: '/',
  algolia: undefined,
  localSearch: undefined,
  translate: undefined,
  noticeOutdate: undefined,
  highlight: {"plugin":"highlighjs","highlightCopy":true,"highlightLang":true,"highlightHeightLimit":200},
  copy: {
    success: '複製成功',
    error: '複製錯誤',
    noSupport: '瀏覽器不支援'
  },
  relativeDate: {
    homepage: false,
    post: false
  },
  runtime: '',
  dateSuffix: {
    just: '剛剛',
    min: '分鐘前',
    hour: '小時前',
    day: '天前',
    month: '個月前'
  },
  copyright: undefined,
  lightbox: 'fancybox',
  Snackbar: undefined,
  infinitegrid: {
    js: 'https://cdn.jsdelivr.net/npm/@egjs/infinitegrid/dist/infinitegrid.min.js',
    buttonText: '載入更多'
  },
  isPhotoFigcaption: true,
  islazyload: false,
  isAnchor: false,
  percent: {
    toc: true,
    rightside: false,
  },
  autoDarkmode: false
}</script><script id="config-diff">var GLOBAL_CONFIG_SITE = {
  title: '極東晝寢愛好家',
  isPost: false,
  isHome: true,
  isHighlightShrink: false,
  isToc: false,
  postUpdate: '2025-08-31 01:30:26'
}</script><noscript><style type="text/css">
  #nav {
    opacity: 1
  }
  #recent-posts time,
  #post-meta time {
    display: inline !important
  }
</style></noscript><script>(win=>{
      win.saveToLocal = {
        set: (key, value, ttl) => {
          if (ttl === 0) return
          const now = Date.now()
          const expiry = now + ttl * 86400000
          const item = {
            value,
            expiry
          }
          localStorage.setItem(key, JSON.stringify(item))
        },
      
        get: key => {
          const itemStr = localStorage.getItem(key)
      
          if (!itemStr) {
            return undefined
          }
          const item = JSON.parse(itemStr)
          const now = Date.now()
      
          if (now > item.expiry) {
            localStorage.removeItem(key)
            return undefined
          }
          return item.value
        }
      }
    
      win.getScript = (url, attr = {}) => new Promise((resolve, reject) => {
        const script = document.createElement('script')
        script.src = url
        script.async = true
        script.onerror = reject
        script.onload = script.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          script.onload = script.onreadystatechange = null
          resolve()
        }

        Object.keys(attr).forEach(key => {
          script.setAttribute(key, attr[key])
        })

        document.head.appendChild(script)
      })
    
      win.getCSS = (url, id = false) => new Promise((resolve, reject) => {
        const link = document.createElement('link')
        link.rel = 'stylesheet'
        link.href = url
        if (id) link.id = id
        link.onerror = reject
        link.onload = link.onreadystatechange = function() {
          const loadState = this.readyState
          if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
          link.onload = link.onreadystatechange = null
          resolve()
        }
        document.head.appendChild(link)
      })
    
      win.activateDarkMode = () => {
        document.documentElement.setAttribute('data-theme', 'dark')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#0d0d0d')
        }
      }
      win.activateLightMode = () => {
        document.documentElement.setAttribute('data-theme', 'light')
        if (document.querySelector('meta[name="theme-color"]') !== null) {
          document.querySelector('meta[name="theme-color"]').setAttribute('content', '#ffffff')
        }
      }
      const t = saveToLocal.get('theme')
    
        if (t === 'dark') activateDarkMode()
        else if (t === 'light') activateLightMode()
      
      const asideStatus = saveToLocal.get('aside-status')
      if (asideStatus !== undefined) {
        if (asideStatus === 'hide') {
          document.documentElement.classList.add('hide-aside')
        } else {
          document.documentElement.classList.remove('hide-aside')
        }
      }
    
      const detectApple = () => {
        if(/iPad|iPhone|iPod|Macintosh/.test(navigator.userAgent)){
          document.documentElement.classList.add('apple')
        }
      }
      detectApple()
    })(window)</script><meta name="generator" content="Hexo 5.4.2"><link rel="alternate" href="/atom.xml" title="極東晝寢愛好家" type="application/atom+xml">
</head><body><div id="sidebar"><div id="menu-mask"></div><div id="sidebar-menus"><div class="avatar-img is-center"><img src="/img/avatar.jpg" onerror="onerror=null;src='/img/friend_404.gif'" alt="avatar"/></div><div class="sidebar-site-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">38</div></a><a href="/tags/"><div class="headline">標籤</div><div class="length-num">20</div></a><a href="/categories/"><div class="headline">分類</div><div class="length-num">4</div></a></div><hr class="custom-hr"/><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首頁</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 歸檔</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 標籤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分類</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友鏈</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://www.travellings.cn/go.html"><i class="fa-fw fas fa-train-subway"></i><span> 開往</span></a></div></div></div></div><div class="page" id="body-wrap"><header class="full_page" id="page-header" style="background-image: url('/img/top_img.jpg')"><nav id="nav"><span id="blog-info"><a href="/" title="極東晝寢愛好家"><span class="site-name">極東晝寢愛好家</span></a></span><div id="menus"><div class="menus_items"><div class="menus_item"><a class="site-page" href="/"><i class="fa-fw fas fa-home"></i><span> 首頁</span></a></div><div class="menus_item"><a class="site-page" href="/archives/"><i class="fa-fw fas fa-archive"></i><span> 歸檔</span></a></div><div class="menus_item"><a class="site-page" href="/tags/"><i class="fa-fw fas fa-tags"></i><span> 標籤</span></a></div><div class="menus_item"><a class="site-page" href="/categories/"><i class="fa-fw fas fa-folder-open"></i><span> 分類</span></a></div><div class="menus_item"><a class="site-page" href="/link/"><i class="fa-fw fas fa-link"></i><span> 友鏈</span></a></div><div class="menus_item"><a class="site-page" target="_blank" rel="noopener" href="https://www.travellings.cn/go.html"><i class="fa-fw fas fa-train-subway"></i><span> 開往</span></a></div></div><div id="toggle-menu"><a class="site-page" href="javascript:void(0);"><i class="fas fa-bars fa-fw"></i></a></div></div></nav><div id="site-info"><h1 id="site-title">極東晝寢愛好家</h1><div id="site_social_icons"><a class="social-icon" href="https://www.linkedin.com/in/leyan-zhu-323abb26a/" target="_blank" title="领英"><i class="fa-brands fa-linkedin"></i></a><a class="social-icon" href="https://www.zhihu.com/people/littlenyima/" target="_blank" title="知乎"><i class="fa-brands fa-zhihu"></i></a><a class="social-icon" href="https://www.cnblogs.com/littlenyima/" target="_blank" title="博客园"><i class="fa-solid fa-blog"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fa-solid fa-rss"></i></a></div></div><div id="scroll-down"><i class="fas fa-angle-down scroll-down-effects"></i></div></header><main class="layout" id="content-inner"><div class="recent-posts" id="recent-posts"><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/posts/top-1-article-index/" title="置顶｜本站文章索引"><i class="fas fa-thumbtack sticky"></i>置顶｜本站文章索引</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">發表於</span><time datetime="2024-07-14T08:08:11.000Z" title="發表於 2024-07-14 16:08:11">2024-07-14</time></span></div><div class="content">生成式人工智能
Flow-based Models

笔记｜Normalizing
Flow 理论与实现（一）基础理论
笔记｜扩散模型（一八）Flow
Matching 理论详解

Diffusion Models：通用理论

笔记｜扩散模型（一）DDPM
理论与实现
笔记｜扩散模型（二）DDIM
理论与实现
笔记｜扩散模型（三）Improved
DDPM 理论与实现
笔记｜扩散模型（四）Classifier
Guidance 理论与实现
笔记｜扩散模型（五）Classifier-Free
Guidance 理论与实现
笔记｜扩散模型（一七）扩散模型中的
Velocity Prediction

Diffusion Models：文生图

笔记｜扩散模型（六）DALL-E
理论与实现｜自回归文生图
笔记｜扩散模型（七）Latent
Diffusion Models（Stable Diffusion）理论与实现
笔记｜扩散模型（八）DALL-E
2 (unCLIP) 理论与实现
笔记｜扩散模型（九）Imagen
理论与实现
笔记｜扩散模型（一〇）Dreambooth
理论与实现｜主题驱动 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/posts/54-training-llms-on-one-gpu/" title="笔记｜大模型训练（一）单卡训练的分析与优化策略">笔记｜大模型训练（一）单卡训练的分析与优化策略</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">發表於</span><time datetime="2025-08-29T15:55:32.000Z" title="發表於 2025-08-29 23:55:32">2025-08-29</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Notes/">Notes</a></span></div><div class="content">
本学习笔记是对 nanotron/ultrascale-playbook
的学习记录，该书涵盖分布式训练、并行技术以及一些优化策略。本文章是该系列笔记的第一篇，对应原书
First Steps: Training on One GPU 一章。

在开始学习分布式训练前，不妨先快速回顾一下模型训练的基础知识。在单
GPU 上训练模型时，训练通常包含三个步骤：

前向传播：将输入传递至模型并产生输出；
反向传播：进行梯度计算；
优化步骤：根据计算出的梯度对模型的参数进行更新。

总体上来说可以用下图表示。图中第一行和最后一行的紫色框可以看作模型的不同层，黑色箭头表示了这些层的连接关系。在训练时，首先对输入进行前向传播（青色箭头），随后反向传播计算梯度（橙色箭头）。使用得到的梯度对模型的参数进行更新，可以得到优化过的模型。

在模型训练过程中，batch size
是最重要的超参数之一，其影响模型训练的收敛速度以及吞吐量。具体来说，在训练初期，较小的
batch size
可以帮助模型快速地到达最佳的学习位置，然而随着训练的进行，小的 batch
size
会导致梯度含有比较多的噪声，最终 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/posts/53-issue-ssl-certificates-with-certbot/" title="技术相关｜使用 Certbot 为通配符域名签发 SSL 证书">技术相关｜使用 Certbot 为通配符域名签发 SSL 证书</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">發表於</span><time datetime="2025-05-26T09:57:41.000Z" title="發表於 2025-05-26 17:57:41">2025-05-26</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Techniques/">Techniques</a></span></div><div class="content">近期在腾讯云给 OSS
配置了自定义域名，并且因为我有两个桶，所以配置了两个不同的子域名。又为了让文件能支持
HTTPS 访问，需要在腾讯云后台给域名配置 SSL 证书。但非常坑的是腾讯云的
SSL 证书不仅需要花钱买而且非常贵，因此为了节能减排最后我选择在 Let's
Encrypt 自己签发一个证书。
Let's Encrypt 推荐的客户端是
Certbot，而为了让多个子域名都能使用同一个证书，需要为通配符域名（也就是一个类似
*.my-domain.com 形式的域名）签发证书，这需要使用 DNS-01
模式进行签发。
从具体操作来说，首先需要访问 Certbot
的官方网站并且选择正确的选项。对于我来说，我并不需要在本地部署证书，因此我选的是
My HTTP website is running Other on Linux
(snap)。选择之后一步步跟着官方的教程安装
certbot，对于我来说命令是下边这些，其他系统的命令可能有所不同：
123sudo apt-get remove certbotsudo snap install --classic certbots ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/posts/52-ddim-inversion-for-cogvideox/" title="开发记录｜基于 CogVideoX 实现 DDIM Inversion">开发记录｜基于 CogVideoX 实现 DDIM Inversion</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">發表於</span><time datetime="2025-02-20T11:44:30.000Z" title="發表於 2025-02-20 19:44:30">2025-02-20</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Techniques/">Techniques</a></span></div><div class="content">近期正在基于 CogVideoX
实现一些视频编辑相关的功能，然而在尝试的时候发现了一个比较奇怪的问题：CogVideoX
无法直接使用和 Stable Diffusion 类似的方式实现 DDIM Inversion。
使用 DDIM 对扩散模型进行采样时，会形成一条「轨迹」。DDIM Inversion
就是从一条现有的视频出发，沿着这条轨迹逆向返回得到最初的噪声的过程。由于
DDIM
采样是确定性的过程，所以从这个得到的噪声出发再重新进行采样，应当能够得到原始视频；如果在采样过程中改变一些控制条件（例如修改视频描述）就可以实现对原始视频的编辑。
为了在 CogVideoX 上实现 DDIM Inversion，最初我的实现大概是这样：
123456789101112131415pipeline = CogVideoXPipeline.from_pretrained(model_path)original_scheduler = pipeline.schedulerinverse_scheduler = DDIMInverseScheduler(**original_schedul ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/posts/51-flow-matching-for-diffusion-models/" title="笔记｜扩散模型（一八）Flow Matching 理论详解">笔记｜扩散模型（一八）Flow Matching 理论详解</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">發表於</span><time datetime="2024-09-20T03:16:52.000Z" title="發表於 2024-09-20 11:16:52">2024-09-20</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Notes/">Notes</a></span></div><div class="content">
论文链接：Flow
Matching for Generative Modeling

在 Stable Diffusion 3 中，模型是通过 Flow Matching
的方法训练的。从这个方法的名字来看，就知道它和 Flow-based Model
有比较强的关联，因此在正式开始介绍这个方法之前先交代一些 Flow-based
Model 相关的背景知识。
Flow-based Models
Normalizing Flow
Normalizing Flow
是一种基于变换对概率分布进行建模的模型，其通过一系列离散且可逆的变换实现任意分布与先验分布（例如标准高斯分布）之间的相互转换。在
Normalizing Flow
训练完成后，就可以直接从高斯分布中进行采样，并通过逆变换得到原始分布中的样本，实现生成的过程。（有关
Normalizing Flow 的详细理论介绍可以移步我的这篇文章观看）
从这个角度看，Normalizing Flow 和 Diffusion Model
是有一些相通的，其做法的对比如下表所示。从表中可以看到，两者大致的过程是非常类似的，尽管依然有些地方不 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/posts/50-velocity-prediction-in-diffusion-models/" title="笔记｜扩散模型（一七）扩散模型中的 Velocity Prediction">笔记｜扩散模型（一七）扩散模型中的 Velocity Prediction</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">發表於</span><time datetime="2024-09-19T08:19:41.000Z" title="發表於 2024-09-19 16:19:41">2024-09-19</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Notes/">Notes</a></span></div><div class="content">
论文链接：Progressive
Distillation for Fast Sampling of Diffusion Models

近期在研究 Rectified Flow 时发现 diffusers 中一个相关的 PR（#5397）训练用到了
Velocity
Prediction，回想起之前某次面试还被问到了这个问题，决定来学习一下这究竟是什么东西。
对 CompVis 实现的 Stable Diffusion 代码比较熟悉的读者应该或多或少都在
scheduler 中读到过这样一段代码：
123456if self.parameterization == &quot;eps&quot;:    target = noiseelif self.parameterization == &quot;x0&quot;:    target = x_startelif self.parameterization == &quot;v&quot;:    target = self.get_v(x_start, noise, t)
这是在计算损失时，也就是 p_loss
函数中的几种 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/posts/48-cogvideox-text-to-video-diffusion-models/" title="笔记｜扩散模型（一六）CogVideoX 论文解读｜文生视频扩散模型">笔记｜扩散模型（一六）CogVideoX 论文解读｜文生视频扩散模型</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">發表於</span><time datetime="2024-09-11T02:18:52.000Z" title="發表於 2024-09-11 10:18:52">2024-09-11</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Notes/">Notes</a></span></div><div class="content">CogVideoX 是智谱近期发布的视频生成模型，和上一个工作 CogVideo
不同，这个方法是基于扩散模型实现的。从框架图来看，感觉 CogVideoX
同时吸取了 Sora 和 Stable Diffusion 3 的优势，不仅使用了 3D
VAE，还引入了双路 DiT 的架构。
具体来说，CogVideoX 主要进行了以下几个方面的工作：

使用 3D VAE 编码视频，有效地压缩视频维度、保证视频的连续性；
引入双路 DiT 分别对文本和视频进行编码，并用 3D attention
进行信息交换；
开发了一个视频标注的 pipeline，用于对视频给出准确的文本标注；
提出了一种渐进式训练方法和一种均匀采样方法。

CogVideoX
CogVideoX 的整体架构如下图所示，文本和视频分别经过文本编码器（这里是
T5）和 3D VAE
编码后输入主干网络。文本和视频分别经过一条支路，并在注意力部分进行交互。

3D Causal VAE
由于视频相比图像多了时序信息，所以需要对多出来的时间维度进行处理。先前的视频生成模型都采用
2D
VAE，这样会导致生成的视频在时间上连续 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/posts/47-cogvideo-text-to-video-generation/" title="笔记｜扩散模型（一五）CogVideo 论文解读｜文生视频大模型">笔记｜扩散模型（一五）CogVideo 论文解读｜文生视频大模型</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">發表於</span><time datetime="2024-09-10T03:02:41.000Z" title="發表於 2024-09-10 11:02:41">2024-09-10</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Notes/">Notes</a></span></div><div class="content">
论文链接：CogVideo:
Large-scale Pretraining for Text-to-Video Generation via
Transformers
官方实现：THUDM/CogVideo（由于目前的仓库里，CogVideo
相关的代码已经被替换为 CogVideoX 的代码，所以如果希望浏览 CogVideo
的代码，包含该方法代码最后一个 commit 为 5f914b7，也就是
这个链接）

和本系列中的 DALL-E 一样，虽然 CogVideo
也并非基于扩散模型的方法，但由于其后续工作 CogVideoX
是基于扩散模型的，所以这篇文章也放到扩散模型系列里。

CogVideo 是基于大规模预训练 Transformer
进行视频生成的工作，也是近期推出的 CogVideoX
的前身。相比于文生图任务，文生视频的主要难点在于两个方面：首先是数据更加稀缺，视频-文本配对数据比较少；其次是视频多了时序信息。
本模型基于文生图模型 CogView2 进行训练，在训练时使用了 5.4 M
视频-文本对数据。在训练时，文本条件是通过 in context
lea ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/posts/35-textual-inversion-personalize-generation/" title="笔记｜扩散模型（一四）Textual Inversion 理论与实现">笔记｜扩散模型（一四）Textual Inversion 理论与实现</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">發表於</span><time datetime="2024-08-07T09:51:27.000Z" title="發表於 2024-08-07 17:51:27">2024-08-07</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Notes/">Notes</a></span></div><div class="content">
论文链接：An Image is
Worth One Word: Personalizing Text-to-Image Generation using Textual
Inversion
官方实现：rinongal/textual_inversion
非官方实现：huggingface/diffusers

Textual Inversion 也是对 diffusion model
进行微调的主要范式之一，从标题中 An Image is Worth One Word
可以猜测，这个方法也是类似 Dreambooth 用某个特别的 text token
来表示所要生成的物体。不过和 Dreambooth 不同的是，Textual Inversion
并不是在 prompt
中插入某个修饰词来表示主体或者风格，而是直接将主体学习为一个
token。这个方法比较特别的是它并不改变原始模型的权重，而只学习了一个额外的
embedding。
Textual Inversion
现有的工作已经证明了 diffusion model 的 text embedding
空间对图像的语义信息有一 ...</div></div></div><div class="recent-post-item"><div class="recent-post-info no-cover"><a class="article-title" href="/posts/32-dit-scalable-diffusion-with-transformers/" title="笔记｜扩散模型（一三）DiT｜Diffusion with Transformer">笔记｜扩散模型（一三）DiT｜Diffusion with Transformer</a><div class="article-meta-wrap"><span class="post-meta-date"><i class="far fa-calendar-alt"></i><span class="article-meta-label">發表於</span><time datetime="2024-08-05T10:12:49.000Z" title="發表於 2024-08-05 18:12:49">2024-08-05</time></span><span class="article-meta"><span class="article-meta-separator">|</span><i class="fas fa-inbox"></i><a class="article-meta__categories" href="/categories/Notes/">Notes</a></span></div><div class="content">
论文链接：Scalable
Diffusion Models with Transformers
官方实现：facebookresearch/DiT

Transformer
在许多领域都有很不错的表现，尤其是近期大语言模型的成功证明了 scaling law
在 NLP 领域的效果。Diffusion Transformer（DiT）把 transformer
架构引入了扩散模型中，并且试图用同样的 scaling
方法提升扩散模型的效果。DiT 提出后就受到了很多后续工作的
follow，例如比较有名的视频生成方法 sora 就采取了 DiT
作为扩散模型的架构。
Diffusion Transformer
在正式开始介绍 DiT 之前，需要先了解一下 DiT 使用的扩散模型架构。DiT
使用的是 latent diffusion，VAE 采用和 Stable Diffusion 相同的
KL-f8，并且使用了 Improved DDPM（详细介绍见这个链接），同时预测噪声的均值和方差。

Patchify
由于 DiT 使用了 latent diffusion，对于 \(256 ...</div></div></div><nav id="pagination"><div class="pagination"><span class="page-number current">1</span><a class="page-number" href="/page/2/#content-inner">2</a><span class="space">&hellip;</span><a class="page-number" href="/page/4/#content-inner">4</a><a class="extend next" rel="next" href="/page/2/#content-inner"><i class="fas fa-chevron-right fa-fw"></i></a></div></nav></div><div class="aside-content" id="aside-content"><div class="card-widget card-info"><div class="is-center"><div class="avatar-img"><img src="/img/avatar.jpg" onerror="this.onerror=null;this.src='/img/friend_404.gif'" alt="avatar"/></div><div class="author-info__name">LittleNyima</div><div class="author-info__description"></div></div><div class="card-info-data site-data is-center"><a href="/archives/"><div class="headline">文章</div><div class="length-num">38</div></a><a href="/tags/"><div class="headline">標籤</div><div class="length-num">20</div></a><a href="/categories/"><div class="headline">分類</div><div class="length-num">4</div></a></div><a id="card-info-btn" target="_blank" rel="noopener" href="https://github.com/LittleNyima"><i class="fab fa-github"></i><span>Follow Me</span></a><div class="card-info-social-icons is-center"><a class="social-icon" href="https://www.linkedin.com/in/leyan-zhu-323abb26a/" target="_blank" title="领英"><i class="fa-brands fa-linkedin"></i></a><a class="social-icon" href="https://www.zhihu.com/people/littlenyima/" target="_blank" title="知乎"><i class="fa-brands fa-zhihu"></i></a><a class="social-icon" href="https://www.cnblogs.com/littlenyima/" target="_blank" title="博客园"><i class="fa-solid fa-blog"></i></a><a class="social-icon" href="/atom.xml" target="_blank" title="RSS"><i class="fa-solid fa-rss"></i></a></div></div><div class="card-widget card-announcement"><div class="item-headline"><i class="fas fa-bullhorn fa-shake"></i><span>公告</span></div><div class="announcement_content">欢迎来到 LittleNyima 的栖息地~（偶尔出没）</div></div><div class="sticky_layout"><div class="card-widget card-recent-post"><div class="item-headline"><i class="fas fa-history"></i><span>最新文章</span></div><div class="aside-list"><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/54-training-llms-on-one-gpu/" title="笔记｜大模型训练（一）单卡训练的分析与优化策略">笔记｜大模型训练（一）单卡训练的分析与优化策略</a><time datetime="2025-08-29T15:55:32.000Z" title="發表於 2025-08-29 23:55:32">2025-08-29</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/53-issue-ssl-certificates-with-certbot/" title="技术相关｜使用 Certbot 为通配符域名签发 SSL 证书">技术相关｜使用 Certbot 为通配符域名签发 SSL 证书</a><time datetime="2025-05-26T09:57:41.000Z" title="發表於 2025-05-26 17:57:41">2025-05-26</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/52-ddim-inversion-for-cogvideox/" title="开发记录｜基于 CogVideoX 实现 DDIM Inversion">开发记录｜基于 CogVideoX 实现 DDIM Inversion</a><time datetime="2025-02-20T11:44:30.000Z" title="發表於 2025-02-20 19:44:30">2025-02-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/51-flow-matching-for-diffusion-models/" title="笔记｜扩散模型（一八）Flow Matching 理论详解">笔记｜扩散模型（一八）Flow Matching 理论详解</a><time datetime="2024-09-20T03:16:52.000Z" title="發表於 2024-09-20 11:16:52">2024-09-20</time></div></div><div class="aside-list-item no-cover"><div class="content"><a class="title" href="/posts/50-velocity-prediction-in-diffusion-models/" title="笔记｜扩散模型（一七）扩散模型中的 Velocity Prediction">笔记｜扩散模型（一七）扩散模型中的 Velocity Prediction</a><time datetime="2024-09-19T08:19:41.000Z" title="發表於 2024-09-19 16:19:41">2024-09-19</time></div></div></div></div><div class="card-widget card-categories"><div class="item-headline">
            <i class="fas fa-folder-open"></i>
            <span>分類</span>
            
            </div>
            <ul class="card-category-list" id="aside-cat-list">
            <li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Jottings/"><span class="card-category-list-name">Jottings</span><span class="card-category-list-count">3</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Notes/"><span class="card-category-list-name">Notes</span><span class="card-category-list-count">24</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Techniques/"><span class="card-category-list-name">Techniques</span><span class="card-category-list-count">8</span></a></li><li class="card-category-list-item "><a class="card-category-list-link" href="/categories/Tutorials/"><span class="card-category-list-name">Tutorials</span><span class="card-category-list-count">2</span></a></li>
            </ul></div><div class="card-widget card-tags"><div class="item-headline"><i class="fas fa-tags"></i><span>標籤</span></div><div class="card-tag-cloud"><a href="/tags/Genetative-models/" style="font-size: 1.1em; color: #999">Genetative models</a> <a href="/tags/Writing/" style="font-size: 1.1em; color: #999">Writing</a> <a href="/tags/Deep-learning/" style="font-size: 1.5em; color: #99a9bf">Deep learning</a> <a href="/tags/Certbot/" style="font-size: 1.1em; color: #999">Certbot</a> <a href="/tags/Linux/" style="font-size: 1.26em; color: #999fa8">Linux</a> <a href="/tags/Generative-models/" style="font-size: 1.42em; color: #99a6b7">Generative models</a> <a href="/tags/Normalizing-flow/" style="font-size: 1.1em; color: #999">Normalizing flow</a> <a href="/tags/Animation-engine/" style="font-size: 1.1em; color: #999">Animation engine</a> <a href="/tags/Pytorch/" style="font-size: 1.18em; color: #999ca1">Pytorch</a> <a href="/tags/Github-Actions/" style="font-size: 1.1em; color: #999">Github Actions</a> <a href="/tags/Hexo/" style="font-size: 1.1em; color: #999">Hexo</a> <a href="/tags/Music-theory/" style="font-size: 1.1em; color: #999">Music theory</a> <a href="/tags/Large-language-models/" style="font-size: 1.1em; color: #999">Large language models</a> <a href="/tags/Python/" style="font-size: 1.1em; color: #999">Python</a> <a href="/tags/SSL/" style="font-size: 1.1em; color: #999">SSL</a> <a href="/tags/Distributed-Computing/" style="font-size: 1.1em; color: #999">Distributed Computing</a> <a href="/tags/Generavie-models/" style="font-size: 1.1em; color: #999">Generavie models</a> <a href="/tags/Security/" style="font-size: 1.1em; color: #999">Security</a> <a href="/tags/Diffusion-models/" style="font-size: 1.34em; color: #99a3b0">Diffusion models</a> <a href="/tags/Manimgl/" style="font-size: 1.1em; color: #999">Manimgl</a></div></div><div class="card-widget card-archives"><div class="item-headline"><i class="fas fa-archive"></i><span>歸檔</span><a class="card-more-btn" href="/archives/" title="檢視更多">
    <i class="fas fa-angle-right"></i></a></div><ul class="card-archive-list"><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/08/"><span class="card-archive-list-date">八月 2025</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/05/"><span class="card-archive-list-date">五月 2025</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2025/02/"><span class="card-archive-list-date">二月 2025</span><span class="card-archive-list-count">1</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/09/"><span class="card-archive-list-date">九月 2024</span><span class="card-archive-list-count">4</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/08/"><span class="card-archive-list-date">八月 2024</span><span class="card-archive-list-count">6</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/07/"><span class="card-archive-list-date">七月 2024</span><span class="card-archive-list-count">9</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/06/"><span class="card-archive-list-date">六月 2024</span><span class="card-archive-list-count">3</span></a></li><li class="card-archive-list-item"><a class="card-archive-list-link" href="/archives/2024/05/"><span class="card-archive-list-date">五月 2024</span><span class="card-archive-list-count">1</span></a></li></ul></div><div class="card-widget card-webinfo"><div class="item-headline"><i class="fas fa-chart-line"></i><span>網站資訊</span></div><div class="webinfo"><div class="webinfo-item"><div class="item-name">文章數目 :</div><div class="item-count">38</div></div><div class="webinfo-item"><div class="item-name">本站訪客數 :</div><div class="item-count" id="busuanzi_value_site_uv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">本站總訪問量 :</div><div class="item-count" id="busuanzi_value_site_pv"><i class="fa-solid fa-spinner fa-spin"></i></div></div><div class="webinfo-item"><div class="item-name">最後更新時間 :</div><div class="item-count" id="last-push-date" data-lastPushDate="2025-08-30T17:30:25.754Z"><i class="fa-solid fa-spinner fa-spin"></i></div></div></div></div></div></div></main><footer id="footer" style="background-image: url('/img/top_img.jpg')"><div id="footer-wrap"><div class="copyright">&copy;2020 - 2025 By LittleNyima</div><div class="framework-info"><span>框架 </span><a target="_blank" rel="noopener" href="https://hexo.io">Hexo</a><span class="footer-separator">|</span><span>主題 </span><a target="_blank" rel="noopener" href="https://github.com/jerryc127/hexo-theme-butterfly">Butterfly</a></div><div class="footer_custom_text">网站封面图作者<a target="_blank" rel="noopener" href="https://www.pixiv.net/artworks/41045442">よこ。</a></div></div></footer></div><div id="rightside"><div id="rightside-config-hide"><button id="darkmode" type="button" title="淺色和深色模式轉換"><i class="fas fa-adjust"></i></button><button id="hide-aside-btn" type="button" title="單欄和雙欄切換"><i class="fas fa-arrows-alt-h"></i></button></div><div id="rightside-config-show"><button id="rightside-config" type="button" title="設定"><i class="fas fa-cog fa-spin"></i></button><button id="go-up" type="button" title="返回頂部"><span class="scroll-percent"></span><i class="fas fa-arrow-up"></i></button></div></div><div><script src="/js/utils.js"></script><script src="/js/main.js"></script><script src="https://cdn.jsdelivr.net/npm/@fancyapps/ui/dist/fancybox/fancybox.umd.min.js"></script><script>function panguFn () {
  if (typeof pangu === 'object') pangu.autoSpacingPage()
  else {
    getScript('https://cdn.jsdelivr.net/npm/pangu/dist/browser/pangu.min.js')
      .then(() => {
        pangu.autoSpacingPage()
      })
  }
}

function panguInit () {
  if (false){
    GLOBAL_CONFIG_SITE.isPost && panguFn()
  } else {
    panguFn()
  }
}

document.addEventListener('DOMContentLoaded', panguInit)</script><div class="js-pjax"></div><script async data-pjax src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script></div></body></html>